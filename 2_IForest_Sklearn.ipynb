{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mT5WFnv2mToJ",
        "y_SBDCCMi9xa",
        "mw1127b7mb6N",
        "in8ZuJzOnWw0",
        "l_Lga0NKY9ui",
        "DJvREJrg3zPx",
        "EAqPEFv3vgiw",
        "0-lZBXbejPVx",
        "VSvvGTLDmLBO",
        "IVd3UT5rm6ei",
        "vHG8B71GppJF",
        "eRrTLeVhmDEh",
        "PLXhwphq5Ub2",
        "OCTz4Dz24lrI",
        "KzD_ei5DxDcB",
        "EGC1u4Uu0cCH",
        "qMPKks8gxOTE",
        "QTYzG21y5E09",
        "jGXTvAWsnoce",
        "Xvgk86afdC74",
        "fLC63tSS5kOe",
        "f-VdEjoL6L1V",
        "ZILcEwHV6WZs"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MiguelCarbo/Anomalies-Detection-TFG/blob/main/2_IForest_Sklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANOMALY EXPLORATION - TFG #\n",
        "### <i> Miguel Ivars Carbo, 795802@unizar.es </i> ###  \n",
        "<i> The main idea of this notebook is to provide a simple and understandable script to be able to perform a basic Isolation forest on a connection log. </i>  \n",
        "<i> Further exploration on this topic must be accomplished in order to obtain the expected results </i>"
      ],
      "metadata": {
        "id": "oznxKykNey0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset Variables\n",
        "%reset"
      ],
      "metadata": {
        "id": "yOaECcElhudS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C:\\Users\\mivar\\Documents\\04_MIGUEL\\01UNIVERSIDAD\\TFG\\Zeek\\Packages\\zat-main\\zat-main\n",
        "\n",
        "# Packet Unzipping\n",
        "!unzip zat.zip;\n",
        "\n",
        "# Local Imports\n",
        "import zat;\n",
        "from zat.log_to_dataframe import LogToDataFrame;\n",
        "from zat.dataframe_to_matrix import DataFrameToMatrix;\n",
        "\n",
        "# Packet Imports\n",
        "import pandas as pd;\n",
        "import numpy as np;\n",
        "from numpy import savetxt;\n",
        "import sklearn;\n",
        "from sklearn.ensemble import IsolationForest;\n",
        "from sklearn.decomposition import PCA;\n",
        "from sklearn.cluster import KMeans, DBSCAN;\n",
        "\n",
        "# Version Printing\n",
        "print('zat: {:s}'.format(zat.__version__))\n",
        "print('Pandas: {:s}'.format(pd.__version__))\n",
        "print('Numpy: {:s}'.format(np.__version__))\n",
        "print('Scikit Learn Version:', sklearn.__version__)"
      ],
      "metadata": {
        "id": "lzs9CkLKfVzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. CHOOSING LOG TO CREATE DATAFRAME FROM ###\n",
        "\n"
      ],
      "metadata": {
        "id": "mT5WFnv2mToJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Pandas dataframe from the Zeek log\n",
        "\n",
        "log_to_df = LogToDataFrame();\n",
        "# // conn_df = log_to_df.create_dataframe('conn.log')\n",
        "conn_df = log_to_df.create_dataframe('mpli_conn_ts.log');\n",
        "\n",
        "# Keep only half of the rows in the DataFrame\n",
        "# // conn_df = conn_df.sample(frac=0.75, random_state=42)\n",
        "print('Read in {:d} Rows...'.format(len(conn_df)))\n",
        "print(conn_df.columns)"
      ],
      "metadata": {
        "id": "PoGlLWJNiNvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. FEATURE SELECTION ###"
      ],
      "metadata": {
        "id": "y_SBDCCMi9xa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.A CONN.LOG ####"
      ],
      "metadata": {
        "id": "mw1127b7mb6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick some features that might be interesting\n",
        "features = ['proto','service','duration','orig_bytes','resp_bytes','conn_state',\n",
        "       'local_orig','local_resp','missed_bytes','history','orig_pkts',\n",
        "       'resp_pkts', 'tunnel_parents'];\n",
        "# features = ['proto', 'service','duration','orig_bytes','resp_bytes','missed_bytes', 'history','orig_pkts','resp_pkts',];   "
      ],
      "metadata": {
        "id": "dpJ7kijKi802"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.B AMPLI_CONN.LOG ####"
      ],
      "metadata": {
        "id": "in8ZuJzOnWw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Features of the ampli_conn.log file\n",
        "\n",
        "# Complete Features\n",
        "# // features = ['ts','sourceAddress','sourcePort','destinationAddress','destinationPort','service','duration','orig_bytes','resp_bytes',\n",
        "#           'history','orig_pkts','resp_pkts','mediaOrigen','mediaResp','desvOrigen','desvResp','mediaTime','desvTime' ]\n",
        "\n",
        "features = ['service','duration','orig_bytes','resp_bytes','history','orig_pkts','resp_pkts','mediaOrigen','mediaResp','desvOrigen','desvResp','mediaTime','desvTime' ]"
      ],
      "metadata": {
        "id": "jbS7_sCGnbNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  3. DATA PREPRATION ####"
      ],
      "metadata": {
        "id": "l_Lga0NKY9ui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1 DATA CLEANSING ####"
      ],
      "metadata": {
        "id": "DJvREJrg3zPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Filtering + NaNs removal\n",
        "conn_features_df = conn_df[features]\n",
        "conn_features_df = conn_features_df.dropna()"
      ],
      "metadata": {
        "id": "pZchMVHar7z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary to convert TimeDelta to Int + DataCleansing\n",
        "for feature in features:\n",
        "    if (conn_features_df[feature].dtype.name == 'category') and ('0' not in conn_features_df[feature].cat.categories):\n",
        "        conn_features_df[feature] = conn_features_df[feature].cat.add_categories(['0'])\n",
        "        conn_features_df[feature] = conn_features_df[feature].fillna('0')\n",
        "    elif pd.api.types.is_timedelta64_dtype(conn_features_df[feature]): \n",
        "        conn_features_df[feature] = conn_features_df[feature].fillna(pd.Timedelta(0))\n",
        "        conn_features_df[feature] = conn_features_df[feature].dt.total_seconds().astype(int)\n",
        "    else: \n",
        "        conn_features_df[feature] = conn_features_df[feature].fillna(0)"
      ],
      "metadata": {
        "id": "9tty2HsmZBQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2 CREATION OF MATRIX BASED ON FEATURES ####"
      ],
      "metadata": {
        "id": "EAqPEFv3vgiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the DataframeToMatrix class (handles categorical data)\n",
        "to_matrix = DataFrameToMatrix();\n",
        "conn_feature_matrix = to_matrix.fit_transform(conn_features_df, normalize=True);\n",
        "conn_feature_matrix.shape\n",
        "# -- Note that we have built the matrix using only the selected features"
      ],
      "metadata": {
        "id": "6nWPObLdvfvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. MODEL TRAINING - ISOLATION FOREST ###\n"
      ],
      "metadata": {
        "id": "0-lZBXbejPVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/fit and Predict anomalous instances using the Isolation Forest model\n",
        "odd_clf = IsolationForest(contamination=0.05) # Normal is around 10% odd\n",
        "odd_clf.fit(conn_feature_matrix)"
      ],
      "metadata": {
        "id": "gtHKNwE1jN7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Now we create a new dataframe using the prediction from our classifier\n",
        "odd_df = conn_features_df[odd_clf.predict(conn_feature_matrix) == -1]\n",
        "# print(odd_df.shape)\n",
        "# odd_df.head()\n",
        "\n",
        "# -- The command is using the trained odd_clf Isolation Forest model to predict \n",
        "# the anomalies in the conn_feature_matrix dataset, then selecting those anomalous \n",
        "# data points from the original conn_df dataset and storing them in the odd_df variable.\n",
        "\n",
        "# -- More specifically, the odd_clf.predict(conn_feature_matrix) == -1 part of \n",
        "# the command applies the Isolation Forest model to the conn_feature_matrix dataset, \n",
        "# and returns an array of predictions with the same length as the number of rows in the dataset. \n",
        "# Each element of the array is either 1 or -1, indicating whether the corresponding data point \n",
        "# is predicted to be normal (1) or anomalous (-1).\n",
        "\n",
        "# Se obtiene el dataframe como matriz\n",
        "odd_matrix = to_matrix.fit_transform(odd_df)"
      ],
      "metadata": {
        "id": "oLOKIVQKjd0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. CLUSTERING THE ODD DATAFRAME ####"
      ],
      "metadata": {
        "id": "VSvvGTLDmLBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.A CLUSTERING WITH KMEANS + DIM.REDUCTION USING PCA / TSNE ####"
      ],
      "metadata": {
        "id": "IVd3UT5rm6ei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 5.A.1 SILHOUETTE SCORING #####"
      ],
      "metadata": {
        "id": "vHG8B71GppJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "scores = []\n",
        "clusters = range(2,14)\n",
        "\n",
        "for K in clusters:\n",
        "    clusterer = KMeans(n_clusters=K)\n",
        "    cluster_labels = clusterer.fit_predict(odd_matrix)\n",
        "    score = silhouette_score(odd_matrix, cluster_labels)\n",
        "    scores.append(score)\n",
        "\n",
        "# -- The first command creates an instance of the KMeans clustering algorithm \n",
        "# with K specified clusters. \n",
        "# KMeans is a popular clustering algorithm that partitions a set of data points \n",
        "# into K clusters based on the similarity of their features. \n",
        "\n",
        "# -- The second command applies the KMeans algorithm to the odd_matrix matrix \n",
        "# of numerical features derived from the anomalous connections. \n",
        "# Specifically, the fit_predict() method of the clusterer object is used \n",
        "# to fit the KMeans model to the data in odd_matrix, and to predict the cluster \n",
        "# labels for each data point in the matrix.\n",
        "\n",
        "# -- The resulting cluster_labels array contains the predicted cluster \n",
        "# labels for each data point in odd_matrix. \n",
        "# The labels are integers ranging from 0 to K-1, indicating the cluster \n",
        "# to which each data point has been assigned by the algorithm.\n",
        "\n",
        "# Plot it out\n",
        "pd.DataFrame({'Num Clusters':clusters, 'score':scores}).plot(x='Num Clusters', y='score')"
      ],
      "metadata": {
        "id": "5P--_shNptCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Same step but with fixed number of clusters\n",
        "kmeans_labels = KMeans(n_clusters=10).fit_predict(odd_matrix)  \n",
        "\n",
        "# 2D - Projection with PCA / TSNE\n",
        "n_components = odd_df.shape[1]\n",
        "projection = PCA(n_components).fit_transform(odd_matrix)\n",
        "# projection = TSNE().fit_transform(odd_matrix);\n",
        "\n",
        "# -- Overall, these commands are performing dimensionality reduction \n",
        "# on the anomalous connections, using PCA to transform the high-dimensional \n",
        "# feature vectors into a lower-dimensional space defined by the principal \n",
        "# components. \n",
        "\n",
        "# Now we can put our ML results back onto our dataframe!\n",
        "odd_df['x'] = projection[:, 0] # Projection X Column\n",
        "odd_df['y'] = projection[:, 1] # Projection Y Column\n",
        "odd_df['cluster'] = kmeans_labels\n",
        "odd_df.head()\n",
        "\n",
        "# -- The first command creates a new column called 'x' in the odd_df dataframe \n",
        "# and assigns to it the values from the first column of the projection matrix, \n",
        "# which correspond to the first principal component of the anomalous connections. \n",
        "# This effectively adds a new column to the dataframe representing \n",
        "# the x-coordinates of each connection in the lower-dimensional space defined \n",
        "# by the principal components.\n",
        "\n",
        "# -- The third command creates a new column called 'cluster' in the odd_df \n",
        "# dataframe and assigns to it the cluster labels obtained from the KMeans algorithm. \n",
        "# This effectively adds a new column to the dataframe indicating the cluster to \n",
        "# which each connection has been assigned.\n",
        "\n",
        "# -- Overall, these commands are adding new columns to the odd_df dataframe \n",
        "# to store the x- and y-coordinates of each anomalous connection in the \n",
        "# lower-dimensional space defined by the principal components, as well as the \n",
        "# cluster labels obtained from the KMeans algorithm. \n",
        "# This can be useful for visualizing the anomalies and the clusters \n",
        "# to which they belong, and for further analyzing the patterns and features \n",
        "# associated with the anomalous behavior."
      ],
      "metadata": {
        "id": "3Jf49Of7kMaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 5.A.2 PLOTTING DEFAULTS #####"
      ],
      "metadata": {
        "id": "eRrTLeVhmDEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Configurations\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['font.size'] = 14.0\n",
        "plt.rcParams['figure.figsize'] = 12.0, 6.0"
      ],
      "metadata": {
        "id": "LC3X-mq9l5Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 5.A.3 GRAPHICAL CLUSTER VISUALIZATION #####"
      ],
      "metadata": {
        "id": "PLXhwphq5Ub2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper method for scatter/beeswarm plot\n",
        "def jitter(arr):\n",
        "    stdev = .02*(max(arr)-min(arr))\n",
        "    return arr + np.random.randn(len(arr)) * stdev\n",
        "    \n",
        "# Jitter so we can see instances that are projected coincident in 2D\n",
        "odd_df['jx'] = jitter(odd_df['x'])\n",
        "odd_df['jy'] = jitter(odd_df['y'])\n",
        "\n",
        "# Now use dataframe group by cluster\n",
        "cluster_groups = odd_df.groupby('cluster')\n",
        "\n",
        "# Plot the Machine Learning results\n",
        "# colors = {0:'green', 1:'blue', 2:'red', 3:'orange', 4:'purple', 5:'brown', 6:'yellow', 7:'magenta', 8:'grey', 9:'cyan', 10:'pink', 11:'olive', 12:'navy', 13:'aquamarine'}\n",
        "colors = {0:'green', 1:'blue', 2:'red', 3:'orange', 4:'purple', 5:'brown', 6:'yellow', 7:'magenta', 8:'grey', 9:'cyan', 10:'pink', 11:'olive'}\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "for key, group in cluster_groups:\n",
        "    group.plot(ax=ax, kind='scatter', x='jx', y='jy', alpha=0.5, s=250,\n",
        "               label='Cluster: {:d}'.format(key), color=colors[key])"
      ],
      "metadata": {
        "id": "0GUyPnTimBKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 5.A.4 CLUSTER DETAIL PRINTING #####"
      ],
      "metadata": {
        "id": "OCTz4Dz24lrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print out the details for each cluster\n",
        "pd.set_option('display.width', 1000)\n",
        "for key, group in cluster_groups:\n",
        "    print('\\nCluster {:d}: {:d} observations'.format(key, len(group)))\n",
        "    print(group[features].head())"
      ],
      "metadata": {
        "id": "d4iKvRU3nHbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.B USING DBSCAN ####"
      ],
      "metadata": {
        "id": "KzD_ei5DxDcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now try DBScan\n",
        "odd_df['cluster_db'] = DBSCAN().fit_predict(odd_matrix)\n",
        "print('Number of Clusters: {:d}'.format(odd_df['cluster_db'].nunique()))"
      ],
      "metadata": {
        "id": "OdQjCndqxMSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 5.B.1 PLOTTING DEFAULTS #####"
      ],
      "metadata": {
        "id": "EGC1u4Uu0cCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Configurations\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['font.size'] = 14.0\n",
        "plt.rcParams['figure.figsize'] = 12.0, 6.0"
      ],
      "metadata": {
        "id": "wW-oPzuN0hJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 5.B.2 CLUSTER RESULTS VISUALIZATION #####"
      ],
      "metadata": {
        "id": "qMPKks8gxOTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE;\n",
        "\n",
        "# Projection using TSNE or PCA algorithm\n",
        "projection = TSNE().fit_transform(odd_matrix);\n",
        "\n",
        "odd_df['x'] = projection[:, 0]; # Projection X Column\n",
        "odd_df['y'] = projection[:, 1]; # Projection Y Column\n",
        "\n",
        "# Helper method for scatter/beeswarm plot\n",
        "def jitter(arr):\n",
        "   stdev = .02*(max(arr)-min(arr))\n",
        "   return arr + np.random.randn(len(arr)) * stdev\n",
        "\n",
        "# Jitter so we can see instances that are projected coincident in 2D\n",
        "odd_df['jx'] = jitter(odd_df['x'])\n",
        "odd_df['jy'] = jitter(odd_df['y'])\n",
        "\n",
        "# Now use dataframe group by cluster\n",
        "cluster_groups_db = odd_df.groupby('cluster_db')\n",
        "\n",
        "# Plot the Machine Learning results\n",
        "# colors = {-1:'green', 0:'blue', 1:'red', 2:'orange', 3:'purple', 4:'brown', 5:'yellow', 6:'magenta', 7:'grey', 8:'cyan', 9:'pink', 10:'olive', 11:'navy', 12:'aquamarine'}\n",
        "colors = {-1:'green', 0:'blue', 1:'red', 2:'orange', 3:'purple'}\n",
        "fig, ax = plt.subplots()\n",
        "for key, group in cluster_groups_db:\n",
        "    group.plot(ax=ax, kind='scatter', x='jx', y='jy', alpha=0.5, s=250,\n",
        "               label='Cluster_DB: {:d}'.format(key), color=colors[key])"
      ],
      "metadata": {
        "id": "K9j41RFRxR2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 5.B.3 CLUSTER RESULTS PRINTING #####"
      ],
      "metadata": {
        "id": "QTYzG21y5E09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now print out the details for each cluster\n",
        "pd.set_option('display.width', 1000)\n",
        "for key, group in cluster_groups_db:\n",
        "    print('\\nCluster {:d}: {:d} observations'.format(key, len(group)))\n",
        "    print(group[features].head())"
      ],
      "metadata": {
        "id": "bSuB1xaJ5KlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. HISTOGRAM OBTENTION ####  \n",
        "<i> Recuerda que esto es válido solo para datos numéricos y no categóricos </i>"
      ],
      "metadata": {
        "id": "jGXTvAWsnoce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of the request body length\n",
        "conn_df[['orig_bytes']].hist()\n",
        "print('\\nFor this small demo dataset almost all request_body_len are 0\\nCluster 2 represents outliers')"
      ],
      "metadata": {
        "id": "t-BgnAR1n66L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. RESULT ANALYSIS ###"
      ],
      "metadata": {
        "id": "kary2jj9mQWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7.A FROM CSV LABELED DATA ####"
      ],
      "metadata": {
        "id": "Xvgk86afdC74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 7.1 ORIGINAL LABELED DATA IMPORTING #####"
      ],
      "metadata": {
        "id": "fLC63tSS5kOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Original Source with Labeled Data\n",
        "\n",
        "original_df = pd.read_csv('Tuesday-WorkingHours.pcap_ISCX.csv')\n",
        "print(original_df.columns)\n",
        "\n",
        "# Drop all rows in which column 'feature' does not fit a criteria\n",
        "malign_original_df = original_df.drop(original_df[original_df[' Label'] == 'BENIGN'].index)\n",
        "\n",
        "# Connection = SrcIP + DstIP\n",
        "if (' Source IP' and ' Destination IP' and ' Source Port' and ' Destination Port') in original_df.columns:\n",
        "    malign_original_df = malign_original_df.rename(columns={' Source IP': 'sourceAddress'})\n",
        "    malign_original_df = malign_original_df.rename(columns={' Destination IP': 'destinationAddress'})\n",
        "    malign_original_df = malign_original_df.rename(columns={' Source Port': 'sourcePort'})\n",
        "    malign_original_df = malign_original_df.rename(columns={' Destination Port': 'destinationPort'})\n",
        "\n",
        "# Unique malign labeled Flows\n",
        "malign_original_df['flux_id'] = malign_original_df['sourceAddress'].astype(str) + malign_original_df['destinationAddress'].astype(str) + malign_original_df['sourcePort'].astype(str) + malign_original_df['destinationPort'].astype(str)\n",
        "malign_flux_id_values = malign_original_df['flux_id'].unique()\n"
      ],
      "metadata": {
        "id": "pRtU1wR35wpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 7.2 ODD DATAFRAME PREPARATION #####"
      ],
      "metadata": {
        "id": "f-VdEjoL6L1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild the odd_df to include original features\n",
        "merged_df = pd.merge(odd_df, conn_df, on=features)\n",
        "\n",
        "# New Field = IP Src + IP Dest + PSrc + PDest\n",
        "merged_df['flux_id'] = merged_df['sourceAddress'].astype(str) + merged_df['destinationAddress'].astype(str) + merged_df['sourcePort'].astype(str) + merged_df['destinationPort'].astype(str)\n",
        "\n",
        "# Unique detected odd Flows\n",
        "odd_flux_id_values = merged_df['flux_id'].unique()\n"
      ],
      "metadata": {
        "id": "gPUt6wLG6KwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 7.3 ANOMALIES COMPARISON #####"
      ],
      "metadata": {
        "id": "ZILcEwHV6WZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Positive Detection\n",
        "detected_positives = 0\n",
        "for value in odd_flux_id_values:\n",
        "\n",
        "    # Check if the concatenated value is in the list of ip_src_dest values\n",
        "    if value in malign_flux_id_values:\n",
        "        detected_positives+=1\n",
        "\n",
        "print('Number of successes:', detected_positives)\n",
        "print('False Positives: ', odd_df.shape[0] - detected_positives)\n",
        "print('Undetected Malign:', malign_original_df.shape[0] - detected_positives)"
      ],
      "metadata": {
        "id": "Vqkty7VImVYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7.B FROM LABELED CONN.LOG ####"
      ],
      "metadata": {
        "id": "cDYwYZmedMaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe etiquetado según el Script creado\n",
        "labeled_conn_df = log_to_df.create_dataframe('mpli_conn_label.log');\n",
        "malign_conn_df = labeled_conn_df[labeled_conn_df['label'] == 'MALIGN'];\n",
        "malign_conn_df['flux_id'] = malign_conn_df['sourceAddress'].astype(str) + malign_conn_df['destinationAddress'].astype(str) + malign_conn_df['sourcePort'].astype(str) + malign_conn_df['destinationPort'].astype(str)"
      ],
      "metadata": {
        "id": "geWz4DyZdUeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only columns with 'MALIGN' label to optimize operations\n",
        "features_ts = features; \n",
        "features_ts.append('ts')\n",
        "odd_features_df = pd.merge(odd_df, conn_df, on=features_ts)\n",
        "odd_features_df['flux_id'] = odd_features_df['sourceAddress'].astype(str) + odd_features_df['destinationAddress'].astype(str) + odd_features_df['sourcePort'].astype(str) + odd_features_df['destinationPort'].astype(str)\n"
      ],
      "metadata": {
        "id": "hra012d4IJwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "malign_common_df_1 = pd.merge(odd_features_df, malign_conn_df, on='flux_id')\n",
        "malign_common_df_2 = pd.merge(odd_features_df, malign_conn_df, on=features)\n",
        "\n",
        "positives1 = len(malign_common_df_1)\n",
        "positives2 = len(malign_common_df_2)\n",
        "\n",
        "print(positives1)\n",
        "print(positives2)"
      ],
      "metadata": {
        "id": "PQxIFka9pJwh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}